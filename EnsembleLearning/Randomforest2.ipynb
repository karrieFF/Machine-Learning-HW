{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate entropy\n",
    "def entropy(data, outcome_label):\n",
    "     \n",
    "    \"\"\"\n",
    "    y is the outcome data series\n",
    "    weight is the weight series\n",
    "    \"\"\"\n",
    "    unique_type = data[outcome_label].value_counts() #update the code to not calculate in a loop\n",
    "    probabilities = unique_type / len(data[outcome_label])\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "#calculate information gain for each feature in each level\n",
    "def information_gain_fun(data, features, outcome_label, n_subset):\n",
    "\n",
    "    \"\"\"\n",
    "    X is the feature data\n",
    "    y is the outcome data series\n",
    "    weight is the weight series\n",
    "    \"\"\"\n",
    "    total_entropy = entropy(data, outcome_label)\n",
    "\n",
    "    len_features = len(features)\n",
    "\n",
    "    if len_features < n_subset:\n",
    "        selected_features = features\n",
    "    else:\n",
    "        selected_features = np.random.choice(features, size = n_subset, replace=False)\n",
    "\n",
    "    output_table = pd.DataFrame(columns = ['factor','decision_value'])\n",
    "\n",
    "    for i1 in range(len(selected_features)):\n",
    "        feature = selected_features[i1]\n",
    "        length_feature = len(data[feature])\n",
    "        feature_categorpy = data[feature].unique()\n",
    "        \n",
    "        expected_entropy = 0\n",
    "\n",
    "        for category in sorted(feature_categorpy):\n",
    "            #subcategory data\n",
    "            sub_data = data[data[feature] == category]\n",
    "\n",
    "            #subcategory entropy\n",
    "            sub_entropy = entropy(sub_data, outcome_label)\n",
    "            proportion= len(sub_data)/length_feature #proportion of one category of a feature\n",
    "            sub_entropy_category = proportion*sub_entropy\n",
    "\n",
    "            #total weight of this feature\n",
    "            expected_entropy += sub_entropy_category\n",
    "\n",
    "        #information gain of this feature\n",
    "        information_gain = total_entropy - expected_entropy\n",
    "\n",
    "        row = {'factor': feature,\"decision_value\":information_gain}\n",
    "        output_table = pd.concat([output_table, pd.DataFrame([row])], ignore_index = True)\n",
    "\n",
    "    # Find the index of the maximum value in the 'decision_value' column\n",
    "    max_index = output_table['decision_value'].idxmax()\n",
    "\n",
    "    # Get the factor with max value\n",
    "    max_factor = output_table.loc[max_index, 'factor']\n",
    "\n",
    "    return max_factor\n",
    "\n",
    "\n",
    "def ID3 (data, features, outcome_label, current_depth, max_depth, n_subset):\n",
    "    trees = {}\n",
    "    \n",
    "    #If all target labels are the same, return label\n",
    "    if data[outcome_label].nunique() == 1:\n",
    "        output_y = data[outcome_label].unique()[0]\n",
    "        return output_y\n",
    "\n",
    "    # If no more features2 are available, return the most common label\n",
    "    elif len(features) == 0 or (current_depth >= max_depth) :\n",
    "        return data[outcome_label].mode()[0]\n",
    "        \n",
    "    else:\n",
    "        best_feature1 = information_gain_fun(data, features, outcome_label, n_subset)\n",
    "        #Innitial tree with best features2:\n",
    "        trees[best_feature1] = {}\n",
    "\n",
    "        for i in data[best_feature1].unique():\n",
    "\n",
    "            split_tree_data = data[data[best_feature1] == i]\n",
    "            \n",
    "            if split_tree_data.empty:\n",
    "                trees[best_feature1][i] = data[outcome_label].mode()[0]\n",
    "\n",
    "            else:\n",
    "                new_features = [f for f in features if f != best_feature1]\n",
    "                update_data = split_tree_data.loc[:, split_tree_data.columns != best_feature1]\n",
    "                subtree = ID3(update_data, new_features, outcome_label, current_depth+1, max_depth, n_subset) #call ID3 again\n",
    "                trees[best_feature1][i] = subtree\n",
    "                \n",
    "        return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one (trees, predictor_data):\n",
    "\n",
    "    if not isinstance(trees, dict):\n",
    "        return trees\n",
    "    \n",
    "    parent_node = next(iter(trees)) #parent node\n",
    "    subtree = trees[parent_node] #subtree of parent_node\n",
    "    node_value = predictor_data[parent_node]  #the value of the parent in the first observation\n",
    "\n",
    "    if node_value in subtree:\n",
    "        return predict_one(subtree[node_value], predictor_data)\n",
    "    \n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def predict(trees, verify_data, features): #test\n",
    "    predict_values = []\n",
    "\n",
    "    for i, row in verify_data.iterrows():\n",
    "        predictors = row[features].to_dict()\n",
    "        predict_value = predict_one(trees, predictors) #predicted y\n",
    "        predict_values.append(predict_value)\n",
    "        \n",
    "    return predict_values\n",
    "    \n",
    "def calculate_error(predictions, true_labels):\n",
    "    incorrect = predictions != true_labels\n",
    "    error = np.sum(incorrect) / len(true_labels)\n",
    "    return error\n",
    "\n",
    "\n",
    "def predict_ensemble(classifiers, data, features):\n",
    "    \"\"\"\n",
    "    Predict using the ensemble of weak learners and their weights (alphas).\n",
    "    \"\"\"\n",
    "    all_tree_predictions = np.zeros((len(classifiers), len(data)))\n",
    "\n",
    "    # Iterate over each classifier and its corresponding alpha value\n",
    "    for i, stump in enumerate(classifiers):\n",
    "        stump_preds = predict(stump, data, features)  # Get the predictions from the current weak classifier\n",
    "\n",
    "        all_tree_predictions[i] = stump_preds\n",
    "\n",
    "    # Combine predictions\n",
    "    final_predictions = np.sign(np.sum(all_tree_predictions, axis=0))\n",
    "\n",
    "    return np.sign(final_predictions)# Return the sign of the weighted sum as the final prediction\n",
    "\n",
    "def calculate_bias_variance(predictions, true_labels):\n",
    "    \n",
    "    # Calculate average predictions across iterations\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias: (Average prediction - True label)^2\n",
    "    bias = np.mean((avg_predictions - true_labels) ** 2)\n",
    "    \n",
    "    # Variance: Variance of predictions across iterations\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regard unknown as a particular attribute\n",
    "def data_preprocessing_attribute(data, features, continuous):\n",
    "    for var in features:\n",
    "        if var in continuous:\n",
    "            media = data[var].median() #replace with median\n",
    "            data[var] = data[var].apply(lambda x:\"no\" if x < media else 'yes')\n",
    "\n",
    "    return data\n",
    "\n",
    "#load data\n",
    "train_data = pd.read_csv(\"D:\\\\EIC-Code\\\\00-Python\\\\Machine-Learning-HW\\\\DecisionTree\\\\bank\\\\train.csv\",header = None, \n",
    "names = ['age','job','marital','education','default','balance','housing','loan','contact','day','month','duration','campaign','pdays','previous','poutcome','y'])\n",
    "\n",
    "test_data = pd.read_csv(\"D:\\\\EIC-Code\\\\00-Python\\\\Machine-Learning-HW\\\\DecisionTree\\\\bank\\\\test.csv\", header = None, \n",
    "names = ['age','job','marital','education','default','balance','housing','loan','contact','day','month','duration','campaign','pdays','previous','poutcome','y'])\n",
    "\n",
    "features = ['age', 'job', 'marital','education', 'default', 'balance', 'housing','loan', 'contact', 'day','month', \n",
    "            'duration','campaign','pdays','previous', 'poutcome']\n",
    "\n",
    "continuous = ['age', 'balance', 'day','duration','campaign','pdays','previous']\n",
    "\n",
    "#load data\n",
    "train_data_att = data_preprocessing_attribute(train_data.copy(), features, continuous)\n",
    "test_data_att = data_preprocessing_attribute(test_data.copy(), features, continuous)\n",
    "\n",
    "train_data_att['y'] = train_data_att['y'].map(lambda label: 1 if label == 'yes' else -1) \n",
    "test_data_att['y'] = test_data_att['y'].map(lambda label: 1 if label == 'yes' else -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outcome_label = 'y'\n",
    "weight_label = 'weight_columns'\n",
    "current_depth = 0\n",
    "max_depth = 2 #len(train_data_att.columns) #range from 1-16\n",
    "n_subsets = [2, 4, 6]\n",
    "n_trees = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m train_error \u001b[38;5;241m=\u001b[39m calculate_error(train_predictions, bootstrap_train_data_att[outcome_label])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#compare true and test in testing dataset\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_att\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m test_error \u001b[38;5;241m=\u001b[39m calculate_error(test_predictions, test_data_att[outcome_label])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_subset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "Cell \u001b[1;32mIn[25], line 40\u001b[0m, in \u001b[0;36mpredict_ensemble\u001b[1;34m(classifiers, data, features)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Iterate over each classifier and its corresponding alpha value\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, stump \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classifiers):\n\u001b[1;32m---> 40\u001b[0m     stump_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstump\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get the predictions from the current weak classifier\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     all_tree_predictions[i] \u001b[38;5;241m=\u001b[39m stump_preds\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Combine predictions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 20\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(trees, verify_data, features)\u001b[0m\n\u001b[0;32m     17\u001b[0m predict_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m verify_data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 20\u001b[0m     predictors \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     21\u001b[0m     predict_value \u001b[38;5;241m=\u001b[39m predict_one(trees, predictors) \u001b[38;5;66;03m#predicted y\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     predict_values\u001b[38;5;241m.\u001b[39mappend(predict_value)\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\series.py:1033\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[1;32m-> 1033\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\series.py:1073\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[1;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1328\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[1;32m-> 1328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28;43misinstance\u001b[39;49m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n\u001b[0;32m   1329\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#random forest algorithms\n",
    "random_trees = []\n",
    "train_errors1 = []\n",
    "train_errors2 = []\n",
    "train_errors3 = []\n",
    "test_errors1 = []\n",
    "test_errors2 = []\n",
    "test_errors3 = []\n",
    "\n",
    "for n_subset in n_subsets:\n",
    "\n",
    "    for i in range(n_trees):\n",
    "\n",
    "        current_depth = 0\n",
    "\n",
    "        # Generate random indices with replacement\n",
    "        indices = np.random.choice(range(len(train_data_att)), size=len(train_data_att), replace=True)\n",
    "\n",
    "        # Create the bootstrap sample using the selected indices\n",
    "        bootstrap_train_data_att = train_data_att.iloc[indices]\n",
    "\n",
    "        decision_stump = ID3 (bootstrap_train_data_att, features, outcome_label, current_depth, max_depth, n_subset)#with only two levels\n",
    "        random_trees.append(decision_stump)\n",
    "\n",
    "        #compare true and test in training dataset\n",
    "        train_predictions = predict_ensemble(random_trees, bootstrap_train_data_att, features)\n",
    "        train_error = calculate_error(train_predictions, bootstrap_train_data_att[outcome_label])\n",
    "    \n",
    "        #compare true and test in testing dataset\n",
    "        test_predictions = predict_ensemble(random_trees, test_data_att, features)\n",
    "        test_error = calculate_error(test_predictions, test_data_att[outcome_label])\n",
    "\n",
    "        if n_subset == 2:\n",
    "            train_errors1.append(1-train_error)\n",
    "            test_errors1.append(1-test_error)\n",
    "\n",
    "        elif n_subset == 4:\n",
    "            train_errors2.append(1-train_error)\n",
    "            test_errors2.append(1-test_error)\n",
    "\n",
    "        else:\n",
    "            train_errors3.append(1-train_error)\n",
    "            test_errors3.append(1-test_error)\n",
    "\n",
    "        print(train_error, test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m     17\u001b[0m     current_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;66;03m#range from 1-16\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Generate random indices with replacement\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     indices1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)), size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, replace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "n_trees = 500\n",
    "n_iterations = 100\n",
    "max_depth = 2 #len(data.columns) #range from 1-16\n",
    "n_subsets = [2, 4, 6]\n",
    "bias1 = []\n",
    "bias2 = []\n",
    "bias3 = []\n",
    "variance1 = []\n",
    "variance2 = []\n",
    "variance3 = []\n",
    "stamp1 = []\n",
    "stamp2 = []\n",
    "stamp3 = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    current_depth = 0\n",
    "    # Generate random indices with replacement\n",
    "    indices = np.random.choice(range(len(train_data_att)), size=1000, replace=True)\n",
    "\n",
    "    # Create the bootstrap sample using the selected indices\n",
    "    sample_train = train_data_att.iloc[indices]\n",
    "    \n",
    "    for n_subset in n_subsets:\n",
    "\n",
    "        for i in range(n_trees):\n",
    "\n",
    "            indices2 = np.random.choice(range(len(sample_train)), size=len(sample_train), replace=True)\n",
    "            \n",
    "            booststrap_train = sample_train.iloc[indices2]\n",
    "\n",
    "            decision_stump = ID3 (booststrap_train, features, outcome_label, current_depth, max_depth, n_subset) #with only two levels\n",
    "            \n",
    "            if i == 0:\n",
    "                predictions_test = predict(decision_stump, test_data_att, features)\n",
    "                bias_single, variance_single = calculate_bias_variance(predictions_test, test_data_att[outcome_label])\n",
    "\n",
    "                if n_subset == 2:\n",
    "                    bias1.append(bias_single)\n",
    "                    variance1.append(1-test_error)\n",
    "                    stamp1.append(decision_stump)\n",
    "\n",
    "                elif n_subset == 4:\n",
    "                    bias2.append(bias_single)\n",
    "                    variance2.append(1-test_error)\n",
    "                    stamp2.append(decision_stump)\n",
    "\n",
    "                else:\n",
    "                    bias3.append(bias_single)\n",
    "                    variance3.append(1-test_error)\n",
    "                    stamp3.append(decision_stump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
