{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate entropy\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    y is the outcome data series\n",
    "    \"\"\"\n",
    "    unique_type = y.value_counts() #update the code to not calculate in a loop\n",
    "    probabilities = unique_type / len(y)\n",
    "    entropy = -np.sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "#calculate information gain for each feature in each level\n",
    "def information_gain_fun(X, y):\n",
    "\n",
    "    \"\"\"\n",
    "    X is the feature data\n",
    "    y is the outcome data series\n",
    "    \"\"\"\n",
    "    total_entropy = entropy(y)\n",
    "    \n",
    "    output_table = pd.DataFrame(columns = ['factor','decision_value'])\n",
    "\n",
    "    features = X.columns\n",
    "    len_features = len(features)\n",
    "\n",
    "    for i1 in range(len_features):\n",
    "        feature = X.columns[i1]\n",
    "        length_feature = len(X[feature])\n",
    "        feature_categorpy = X[feature].unique()\n",
    "        \n",
    "        expected_entropy = 0\n",
    "\n",
    "        for category in sorted(feature_categorpy):\n",
    "            \n",
    "            #subcategory data\n",
    "            sub_feature = X[X[feature] == category]\n",
    "            sub_y = y[sub_feature.index]\n",
    "\n",
    "            #subcategory entropy\n",
    "            sub_entropy = entropy(sub_y)\n",
    "            proportion= len(sub_feature)/length_feature #proportion of one category of a feature\n",
    "            sub_entropy_category = proportion*sub_entropy\n",
    "\n",
    "            #total weight of this feature\n",
    "            expected_entropy += sub_entropy_category\n",
    "\n",
    "        #information gain of this feature\n",
    "        information_gain = total_entropy - expected_entropy\n",
    "\n",
    "        row = {'factor': feature,\"decision_value\":information_gain}\n",
    "        output_table = pd.concat([output_table, pd.DataFrame([row])], ignore_index = True)\n",
    "\n",
    "    # Find the index of the maximum value in the 'decision_value' column\n",
    "    max_index = output_table['decision_value'].idxmax()\n",
    "\n",
    "    # Get the factor with max value\n",
    "    max_factor = output_table.loc[max_index, 'factor']\n",
    "\n",
    "    return max_factor\n",
    "\n",
    "def ID3 (X, y, current_depth, max_depth):\n",
    "    trees = {}\n",
    "    features2 = X.columns\n",
    "    \n",
    "    #If all target labels are the same, return label\n",
    "    if y.nunique() == 1:\n",
    "        output_y = y.unique()[0]\n",
    "        return output_y\n",
    "\n",
    "    # If no more features2 are available, return the most common label\n",
    "    elif len(features2) == 0 or (current_depth >= max_depth) :\n",
    "        return y.mode()[0]\n",
    "        \n",
    "    else:\n",
    "        best_feature1 = information_gain_fun(X, y)\n",
    "        #Innitial tree with best features2:\n",
    "        trees[best_feature1] = {}\n",
    "\n",
    "        for i in X[best_feature1].unique():\n",
    "\n",
    "            split_tree_data = X[X[best_feature1] == i]\n",
    "            if split_tree_data.empty:\n",
    "                trees[best_feature1][i] = y.mode()[0]\n",
    "\n",
    "            else:\n",
    "                update_X = split_tree_data.loc[:, split_tree_data.columns != best_feature1]\n",
    "                subtree = ID3(update_X, y, current_depth+1, max_depth) #call ID3 again\n",
    "                trees[best_feature1][i] = subtree\n",
    "        return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(trees, predictor_data):\n",
    "\n",
    "    if not isinstance(trees, dict):\n",
    "        return trees\n",
    "    \n",
    "    parent_node = next(iter(trees)) #parent node\n",
    "    subtree = trees[parent_node] #subtree of parent_node\n",
    "    node_value = predictor_data[parent_node]  #the value of the parent in the first observation\n",
    "\n",
    "    if node_value in subtree:\n",
    "        return predict(subtree[node_value], predictor_data)\n",
    "    \n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def calculate_error(predictions, true_labels):\n",
    "    incorrect = predictions != true_labels\n",
    "    error = np.sum(incorrect) / len(true_labels)\n",
    "    return error\n",
    "\n",
    "def predict_ensemble(bagging_trees, X_train):\n",
    "\n",
    "    all_tree_predictions = np.zeros((len(bagging_trees), len(X_train)))\n",
    "\n",
    "    for i, stump in enumerate(bagging_trees):\n",
    "        stump_preds = X_train.apply(lambda row: predict(stump, row), axis=1)\n",
    "        all_tree_predictions[i] = stump_preds\n",
    "\n",
    "    # Combine predictions\n",
    "    final_predictions = np.sign(np.sum(all_tree_predictions, axis=0))\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "def calculate_bias_variance(predictions, true_labels):\n",
    "    \n",
    "    # Calculate average predictions across iterations\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias: (Average prediction - True label)^2\n",
    "    bias = np.mean((avg_predictions - true_labels) ** 2)\n",
    "    \n",
    "    # Variance: Variance of predictions across iterations\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#regard unknown as a particular attribute\n",
    "def data_preprocessing_attribute(data, features, continuous):\n",
    "    for var in features:\n",
    "        if var in continuous:\n",
    "            media = data[var].median() #replace with median\n",
    "            data[var] = data[var].apply(lambda x:\"no\" if x < media else 'yes')\n",
    "\n",
    "    return data\n",
    "\n",
    "#load data\n",
    "train_data = pd.read_csv(\"D:\\\\EIC-Code\\\\00-Python\\\\Machine-Learning-HW\\\\DecisionTree\\\\bank\\\\train.csv\",header = None, \n",
    "names = ['age','job','marital','education','default','balance','housing','loan','contact','day','month','duration','campaign','pdays','previous','poutcome','y'])\n",
    "\n",
    "test_data = pd.read_csv(\"D:\\\\EIC-Code\\\\00-Python\\\\Machine-Learning-HW\\\\DecisionTree\\\\bank\\\\test.csv\", header = None, \n",
    "names = ['age','job','marital','education','default','balance','housing','loan','contact','day','month','duration','campaign','pdays','previous','poutcome','y'])\n",
    "\n",
    "features = ['age', 'job', 'marital','education', 'default', 'balance', 'housing','loan', 'contact', 'day','month', \n",
    "            'duration','campaign','pdays','previous', 'poutcome']\n",
    "\n",
    "continuous = ['age', 'balance', 'day','duration','campaign','pdays','previous']\n",
    "\n",
    "#load data\n",
    "train_data_att = data_preprocessing_attribute(train_data.copy(), features, continuous)\n",
    "test_data_att = data_preprocessing_attribute(test_data.copy(), features, continuous)\n",
    "\n",
    "X_train = train_data_att[features]\n",
    "y_train = train_data_att['y'].map(lambda label: 1 if label == 'yes' else -1) \n",
    "\n",
    "X_test = test_data_att[features]\n",
    "y_test = test_data_att['y'].map(lambda label: 1 if label == 'yes' else -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging trees\n",
    "bagging_trees = []\n",
    "n_trees = 500\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for i in range(n_trees):\n",
    "\n",
    "    current_depth = 0\n",
    "    max_depth = len(X_train.columns) #range from 1-16\n",
    "\n",
    "    # Generate random indices with replacement\n",
    "    indices = np.random.choice(range(len(X_train)), size=len(X_train), replace=True)\n",
    "\n",
    "    # Create the bootstrap sample using the selected indices\n",
    "    X_bootstrap_train = X_train.iloc[indices]\n",
    "    y_bootstrap_train = y_train.iloc[indices]\n",
    "    \n",
    "    decision_stump = ID3(X_bootstrap_train, y_bootstrap_train, current_depth, max_depth) #with only two levels\n",
    "    \n",
    "    #compare true and test in training dataset\n",
    "    train_predictions = predict_ensemble(bagging_trees, X_bootstrap_train)\n",
    "    train_error = calculate_error(train_predictions, y_bootstrap_train)\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "    #compare true and test in testing dataset\n",
    "    test_predictions = predict_ensemble(bagging_trees, X_test)\n",
    "    test_error = calculate_error(test_predictions, y_test)\n",
    "    test_errors.append(test_error)\n",
    "    print(train_error, test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m X_bootstrap_train \u001b[38;5;241m=\u001b[39m X_sample_train\u001b[38;5;241m.\u001b[39miloc[indices2]\n\u001b[0;32m     26\u001b[0m y_bootstrap_train \u001b[38;5;241m=\u001b[39m y_sample_train\u001b[38;5;241m.\u001b[39miloc[indices2]\n\u001b[1;32m---> 28\u001b[0m decision_stump \u001b[38;5;241m=\u001b[39m \u001b[43mID3\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_bootstrap_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bootstrap_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#with only two levels\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m     predictions_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: predict(decision_stump, row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[65], line 87\u001b[0m, in \u001b[0;36mID3\u001b[1;34m(X, y, current_depth, max_depth)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         update_X \u001b[38;5;241m=\u001b[39m split_tree_data\u001b[38;5;241m.\u001b[39mloc[:, split_tree_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m!=\u001b[39m best_feature1]\n\u001b[1;32m---> 87\u001b[0m         subtree \u001b[38;5;241m=\u001b[39m \u001b[43mID3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#call ID3 again\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         trees[best_feature1][i] \u001b[38;5;241m=\u001b[39m subtree\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trees\n",
      "Cell \u001b[1;32mIn[65], line 87\u001b[0m, in \u001b[0;36mID3\u001b[1;34m(X, y, current_depth, max_depth)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         update_X \u001b[38;5;241m=\u001b[39m split_tree_data\u001b[38;5;241m.\u001b[39mloc[:, split_tree_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m!=\u001b[39m best_feature1]\n\u001b[1;32m---> 87\u001b[0m         subtree \u001b[38;5;241m=\u001b[39m \u001b[43mID3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#call ID3 again\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         trees[best_feature1][i] \u001b[38;5;241m=\u001b[39m subtree\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trees\n",
      "    \u001b[1;31m[... skipping similar frames: ID3 at line 87 (5 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[65], line 87\u001b[0m, in \u001b[0;36mID3\u001b[1;34m(X, y, current_depth, max_depth)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         update_X \u001b[38;5;241m=\u001b[39m split_tree_data\u001b[38;5;241m.\u001b[39mloc[:, split_tree_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m!=\u001b[39m best_feature1]\n\u001b[1;32m---> 87\u001b[0m         subtree \u001b[38;5;241m=\u001b[39m \u001b[43mID3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#call ID3 again\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         trees[best_feature1][i] \u001b[38;5;241m=\u001b[39m subtree\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trees\n",
      "Cell \u001b[1;32mIn[65], line 75\u001b[0m, in \u001b[0;36mID3\u001b[1;34m(X, y, current_depth, max_depth)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mmode()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     best_feature1 \u001b[38;5;241m=\u001b[39m \u001b[43minformation_gain_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m#Innitial tree with best features2:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     trees[best_feature1] \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[65], line 29\u001b[0m, in \u001b[0;36minformation_gain_fun\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     27\u001b[0m feature \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns[i1]\n\u001b[0;32m     28\u001b[0m length_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X[feature])\n\u001b[1;32m---> 29\u001b[0m feature_categorpy \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m expected_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(feature_categorpy):\n\u001b[0;32m     34\u001b[0m     \n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m#subcategory data\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\series.py:2194\u001b[0m, in \u001b[0;36mSeries.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\base.py:1030\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1030\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\algorithms.py:390\u001b[0m, in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\pandas\\core\\algorithms.py:429\u001b[0m, in \u001b[0;36munique_with_mask\u001b[1;34m(values, mask)\u001b[0m\n\u001b[0;32m    427\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 429\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "largest_bagging_trees = []\n",
    "bias_singles = []\n",
    "variance_singles = []\n",
    "bias_alls = []\n",
    "variance_alls = []\n",
    "n_trees = 500\n",
    "n_iterations = 100\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    current_depth = 0\n",
    "    max_depth = len(X_train.columns) #range from 1-16\n",
    "\n",
    "    # Generate random indices with replacement\n",
    "    indices1 = np.random.choice(range(len(X_train)), size = 1000, replace = True)\n",
    "\n",
    "    # Create the bootstrap sample using the selected indices\n",
    "    X_sample_train = X_train.iloc[indices1]\n",
    "    y_sample_train = y_train.iloc[indices1]\n",
    "\n",
    "    for i in range(n_trees):\n",
    "\n",
    "        indices2 = np.random.choice(range(len(X_sample_train)), size=len(X_sample_train), replace=True)\n",
    "        \n",
    "        X_bootstrap_train = X_sample_train.iloc[indices2]\n",
    "        y_bootstrap_train = y_sample_train.iloc[indices2]\n",
    "\n",
    "        decision_stump = ID3 (X_bootstrap_train, y_bootstrap_train, current_depth, max_depth) #with only two levels\n",
    "        \n",
    "        if i == 0:\n",
    "            predictions_test = X_test.apply(lambda row: predict(decision_stump, row), axis=1)\n",
    "            incorrect_train = predictions_test != y_test\n",
    "\n",
    "            bias_single, variance_single = calculate_bias_variance (predictions_test, y_test)\n",
    "\n",
    "            bias_singles.append(bias_single)\n",
    "            variance_singles.append(variance_single)\n",
    "\n",
    "            largest_bagging_trees.append(decision_stump)\n",
    "\n",
    "all_predictions = predict_ensemble(largest_bagging_trees, X_test)\n",
    "all_bias, all_variance = calculate_bias_variance (all_predictions, y_test)\n",
    "bias_alls.append(all_bias)\n",
    "variance_alls.append(all_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
