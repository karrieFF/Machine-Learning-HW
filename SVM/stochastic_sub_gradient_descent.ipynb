{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload dataset\n",
    "train_data = pd.read_csv(\"D:/EIC-Code/00-Python/Machine-Learning-HW/SVM/bank-note/train.csv\", header = None, names = ['variance','skewness','curtosis','entropy','y'])\n",
    "test_data = pd.read_csv(\"D:/EIC-Code/00-Python/Machine-Learning-HW/SVM/bank-note/test.csv\", names = ['variance','skewness','curtosis','entropy','y'])\n",
    "\n",
    "features = ['variance','skewness','curtosis','entropy']\n",
    "outcome = 'y'\n",
    "\n",
    "X_train = train_data[features].values #change to matrix multiple\n",
    "y_train = train_data[outcome].values\n",
    "X_test = test_data[features].values\n",
    "y_test = test_data[outcome].values\n",
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrimalSVM:\n",
    "#     def __init__(self, gamma, a, C, N):\n",
    "#         self.gamma = gamma\n",
    "#         self.a = a\n",
    "#         self.C = C\n",
    "#         self.N = N\n",
    "#         self.w = None  # Weight vector will be initialized during training\n",
    "        \n",
    "\n",
    "#     def fit(self, X, y, epochs, schedule):\n",
    "#         n_features = X.shape[1]\n",
    "#         self.w = np.zeros(n_features)  # Initialize weights to zeros\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             data = np.column_stack((X, y))\n",
    "#             np.random.shuffle(data)  # Shuffle training data\n",
    "#             X_shuffled = data[:, :-1]\n",
    "#             y_shuffled = data[:, -1]\n",
    "\n",
    "#             for i, (xi, yi) in enumerate(zip(X_shuffled, y_shuffled)):\n",
    "#                 t = epoch * len(y_shuffled) + i + 1  # Global step count\n",
    "                \n",
    "#                 # Learning rate schedules\n",
    "#                 if schedule == \"schedule1\":\n",
    "#                     gamma_t = self.gamma / (1 + (self.gamma / self.a) * t)\n",
    "#                 elif schedule == \"schedule2\":\n",
    "#                     gamma_t = self.gamma / (1 + t)\n",
    "#                 else:\n",
    "#                     raise ValueError(\"Invalid schedule. Choose 'schedule1' or 'schedule2'.\")\n",
    "\n",
    "#                 if yi * np.dot(self.w, xi) <= 1:\n",
    "#                     self.w = self.w - gamma_t * self.w + gamma_t * self.C * self.N * yi * xi\n",
    "#                 else:\n",
    "#                     self.w = (1 - gamma_t) * self.w\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return np.sign(X.dot(self.w))\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         predictions = self.predict(X)\n",
    "#         errors = np.mean(predictions != y)\n",
    "#         return errors\n",
    "# # np.random.seed(0)\n",
    "# # # Define hyperparameter search space\n",
    "# # Cs = [100 / 873, 500 / 873, 700 / 873]\n",
    "# # gamma_values = [0.1, 0.01, 0.001]#\n",
    "# # a_values = [1, 10, 100]\n",
    "# # N = 1\n",
    "# # epochs = 100\n",
    "\n",
    "\n",
    "# # lowest_error = 10000 #float(\"inf\")\n",
    "# # best_params = None\n",
    "\n",
    "# # for C in Cs:\n",
    "# #     for gamma in gamma_values:\n",
    "# #         for a in a_values:\n",
    "# #             svm = PrimalSVM(gamma=gamma, a=a, C=C, N=N)\n",
    "            \n",
    "# #             svm.fit(X_train, y_train, epochs=epochs, schedule=\"schedule2\") #schedule 1 and schedule 2\n",
    "# #             train_errors = svm.score(X_train, y_train)\n",
    "# #             test_errors = svm.score(X_test, y_test)\n",
    "# #             print(f\"Testing: C={C}, gamma={gamma}, a={a}, train_error: {train_errors}, test_error:{test_errors}\")\n",
    "\n",
    "# #             if test_errors < lowest_error:\n",
    "# #                 lowest_error = test_errors\n",
    "# #                 best_params = (C, gamma, a)\n",
    "\n",
    "# # print(f\"Best parameters: C={best_params[0]}, gamma={best_params[1]}, a={best_params[2]}\")\n",
    "# # print(f\"Lowest error: {lowest_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrimalSVM:\n",
    "    def __init__(self, gamma, a, C, N):\n",
    "        self.gamma = gamma\n",
    "        self.a = a\n",
    "        self.C = C\n",
    "        self.N = N\n",
    "        self.w = None  # Weight vector\n",
    "        self.b = 0  # Bias term\n",
    "        self.objective_curve = []  # Stores hinge loss at each epoch\n",
    "\n",
    "    def _hinge_loss(self, X, y):\n",
    "        loss = 0.5 * np.dot(self.w, self.w) + self.C * np.sum(np.maximum(0, 1 - y * (np.dot(X, self.w) + self.b))) #objective function\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs, schedule):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.w = np.zeros(n_features)  # Initialize weights\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            perm = np.random.permutation(n_samples)\n",
    "            X_train, y_train = X_train[perm], y_train[perm]\n",
    "\n",
    "            for i, (xi, yi) in enumerate(zip(X_train, y_train)):\n",
    "                t = epoch * n_samples + i + 1  # Global step count\n",
    "\n",
    "                # Learning rate schedule\n",
    "                if schedule == \"schedule1\":\n",
    "                    eta_t = self.gamma / (1 + (self.gamma / self.a) * t)\n",
    "                elif schedule == \"schedule2\":\n",
    "                    eta_t = self.gamma / (1 + t)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid schedule. Choose 'schedule1' or 'schedule2'.\")\n",
    "\n",
    "                # Sub-gradient updates\n",
    "                if yi * (np.dot(self.w, xi) + self.b) <= 1:\n",
    "                    self.w = (1 - eta_t) * self.w + eta_t * self.C * self.N * yi * xi\n",
    "                    self.b += eta_t * self.C * yi\n",
    "                else:\n",
    "                    self.w = (1 - eta_t) * self.w\n",
    "\n",
    "            # Compute hinge loss at the end of each epoch\n",
    "            self.objective_curve.append(self._hinge_loss(X_train, y_train))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)\n",
    "\n",
    "    def get_objective_curve(self):\n",
    "        return self.objective_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "Cs = [100 / 873, 500 / 873, 700 / 873]\n",
    "gamma_values = [0.1, 0.01, 0.001]\n",
    "a_values = [1, 10, 100]\n",
    "N = 1\n",
    "epochs = 100\n",
    "\n",
    "# Initialize variables to track the best parameters and lowest error\n",
    "lowest_error = float(\"inf\")\n",
    "best_params = None\n",
    "best_schedule = None\n",
    "\n",
    "# Perform grid search\n",
    "for schedule in [\"schedule1\", \"schedule2\"]:\n",
    "    for C in Cs:\n",
    "        for gamma in gamma_values:\n",
    "            for a in a_values:\n",
    "                # Initialize and train the SVM\n",
    "                svm = PrimalSVM(gamma=gamma, a=a, C=C, N=N)\n",
    "                svm.fit(X_train, y_train, epochs=epochs, schedule=schedule)\n",
    "\n",
    "                # Calculate training and test errors\n",
    "                train_error = svm.score(X_train, y_train)\n",
    "                test_error = svm.score(X_test, y_test)\n",
    "                print(f\"Schedule: {schedule}, C={C:.6f}, gamma={gamma}, a={a}, \"\n",
    "                      f\"train_error: {train_error:.4f}, test_error: {test_error:.4f}\")\n",
    "\n",
    "                # Update the best parameters if a lower test error is found\n",
    "                if test_error < lowest_error:\n",
    "                    lowest_error = test_error\n",
    "                    best_params = (C, gamma, a)\n",
    "                    best_schedule = schedule\n",
    "\n",
    "# Print the best parameters and the lowest error\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"Schedule: {best_schedule}, C={best_params[0]:.6f}, gamma={best_params[1]}, a={best_params[2]}\")\n",
    "print(f\"Lowest Test Error: {lowest_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class DualSVM:\n",
    "    def __init__(self, C):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.alpha = None  # Lagrange multipliers\n",
    "        self.w = None  # Weight vector\n",
    "        self.b = None  # Bias term\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # kernel matrix (linear kernel)\n",
    "        K = np.dot(X, X.T)\n",
    "\n",
    "        # define the dual objective function\n",
    "        def objective(alpha):\n",
    "            return -np.sum(alpha) + 0.5 * np.sum((alpha * y)[:, None] * (alpha * y) * K)\n",
    "\n",
    "        # Initial guess for alpha\n",
    "        alpha0 = np.zeros(n_samples)\n",
    "\n",
    "        # Bounds for alpha: 0 <= alpha <= C\n",
    "        bounds = [(0, C) for _ in range(n_samples)]\n",
    "\n",
    "        # Equality constraint: sum(alpha * y) = 0\n",
    "        constraints = {\n",
    "            'type': 'eq',\n",
    "            'fun': lambda alpha: np.dot(alpha, y),\n",
    "            'jac': lambda alpha: y\n",
    "            }\n",
    "        \n",
    "        # Solve the optimization problem\n",
    "        result = minimize(\n",
    "            objective,\n",
    "            alpha0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints\n",
    "        )\n",
    "\n",
    "        # extract the optimal alpha\n",
    "        self.alpha = result.x #minimize the function to get alpha\n",
    "\n",
    "        # compute weight vector\n",
    "        self.w = np.sum((self.alpha * y)[:, None] * X, axis=0)\n",
    "\n",
    "        # compute bias term\n",
    "        support_vector_idx = np.where((self.alpha > 0) & (self.alpha < self.C))[0][0]\n",
    "        self.b = y[support_vector_idx] - np.dot(self.w, X[support_vector_idx])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.1145475372279496, Train Accuracy=0.11, Test Accuracy=0.12\n",
      "Weights:, [-0.94292598 -0.65149184 -0.73372197 -0.04102195], Bias:, 4.14119177534919\n",
      "C=0.572737686139748, Train Accuracy=0.15, Test Accuracy=0.14\n",
      "Weights:, [-1.56393784 -1.01405165 -1.18065044 -0.15651687], Bias:, 7.590350666124916\n",
      "C=0.8018327605956472, Train Accuracy=0.40, Test Accuracy=0.43\n",
      "Weights:, [-2.04254833 -1.28068891 -1.51351532 -0.24905307], Bias:, 12.975949611428163\n",
      "Weights: [-2.04254833 -1.28068891 -1.51351532 -0.24905307]\n",
      "Bias: 12.975949611428163\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Define hyperparameter C\n",
    "Cs = [100 / 873, 500 / 873, 700 / 873] #, 873\n",
    "\n",
    "# Train and evaluate the model for different values of C\n",
    "for C in Cs:\n",
    "    svm = DualSVM(C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    train_error = svm.score(X_train, y_train)\n",
    "    test_error = svm.score(X_test, y_test)\n",
    "    print(f\"C={C}, Train error={train_error:.2f}, Test error={test_error:.2f}\")\n",
    "    print(f\"Weights:, {svm.w}, Bias:, {svm.b}\")\n",
    "\n",
    "# Print the weights and bias for the best model\n",
    "print(\"Weights:\", svm.w)\n",
    "print(\"Bias:\", svm.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GaussianSVM:\n",
    "    def __init__(self, C, gamma):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.alpha = None  # lagrange multipliers\n",
    "        self.b = None  # bias term\n",
    "        self.X_train = None  # training features\n",
    "        self.y_train = None  # training labels\n",
    "\n",
    "    def gaussian_kernel(self, x1, x2):\n",
    "        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / self.gamma)\n",
    "\n",
    "    def kernel_matrix(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i, j] = self.gaussian_kernel(X[i], X[j])\n",
    "        return K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Compute the kernel matrix\n",
    "        K = self.kernel_matrix(X)\n",
    "\n",
    "        # Define the dual objective function\n",
    "        def objective(alpha):\n",
    "            return -np.sum(alpha) + 0.5 * np.sum((alpha * y)[:, None] * (alpha * y) * K)\n",
    "\n",
    "        # Bounds for alpha: 0 <= alpha <= C\n",
    "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
    "\n",
    "        # Equality constraint: sum(alpha * y) = 0\n",
    "        constraints = {\n",
    "            'type': 'eq',\n",
    "            'fun': lambda alpha: np.dot(alpha, y),\n",
    "            'jac': lambda alpha: y\n",
    "        }\n",
    "\n",
    "        # Initial guess for alpha\n",
    "        alpha0 = np.zeros(n_samples)\n",
    "\n",
    "        # Solve the optimization problem\n",
    "        result = minimize(\n",
    "            objective,\n",
    "            alpha0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            options={'maxiter': 1000, 'disp': True}\n",
    "        )\n",
    "\n",
    "        # Extract the optimal alpha\n",
    "        self.alpha = result.x\n",
    "\n",
    "        # Compute bias term using support vectors\n",
    "        support_vector_idx = np.where((self.alpha > 1e-4) & (self.alpha < self.C))[0]\n",
    "        support_vector_idx = support_vector_idx[0]\n",
    "        self.b = y[support_vector_idx] - np.sum(self.alpha * y * K[support_vector_idx])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            # Decision function\n",
    "            decision = np.sum(\n",
    "                self.alpha * self.y_train *\n",
    "                np.array([self.gaussian_kernel(x, x_train) for x_train in self.X_train])\n",
    "            ) + self.b\n",
    "            y_pred.append(np.sign(decision))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)  # Error rate\n",
    "\n",
    "    def get_support_vectors(self):\n",
    "        return np.where((self.alpha > 1e-4) & (self.alpha < self.C))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -82.72279493439663\n",
      "            Iterations: 15\n",
      "            Function evaluations: 13098\n",
      "            Gradient evaluations: 15\n",
      "C=0.1145, gamma=0.1, Train error=0.4461, Test error=0.4420\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -74.16551934430456\n",
      "            Iterations: 23\n",
      "            Function evaluations: 20084\n",
      "            Gradient evaluations: 23\n",
      "C=0.1145, gamma=0.5, Train error=0.4071, Test error=0.4260\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -63.41453356303385\n",
      "            Iterations: 25\n",
      "            Function evaluations: 21830\n",
      "            Gradient evaluations: 25\n",
      "C=0.1145, gamma=1, Train error=0.2144, Test error=0.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -26.45826553383153\n",
      "            Iterations: 36\n",
      "            Function evaluations: 31437\n",
      "            Gradient evaluations: 36\n",
      "C=0.1145, gamma=5, Train error=0.0034, Test error=0.0040\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -20.289226978400574\n",
      "            Iterations: 26\n",
      "            Function evaluations: 22703\n",
      "            Gradient evaluations: 26\n",
      "C=0.1145, gamma=100, Train error=0.0138, Test error=0.0080\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -285.8328142581335\n",
      "            Iterations: 19\n",
      "            Function evaluations: 16590\n",
      "            Gradient evaluations: 19\n",
      "C=0.5727, gamma=0.1, Train error=0.0000, Test error=0.3480\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -159.75505159853384\n",
      "            Iterations: 39\n",
      "            Function evaluations: 34053\n",
      "            Gradient evaluations: 39\n",
      "C=0.5727, gamma=0.5, Train error=0.0000, Test error=0.0180\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -102.93934454458342\n",
      "            Iterations: 53\n",
      "            Function evaluations: 46277\n",
      "            Gradient evaluations: 53\n",
      "C=0.5727, gamma=1, Train error=0.0000, Test error=0.0040\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.227984451198736\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55010\n",
      "            Gradient evaluations: 63\n",
      "C=0.5727, gamma=5, Train error=0.0000, Test error=0.0000\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -39.83017265028401\n",
      "            Iterations: 53\n",
      "            Function evaluations: 46277\n",
      "            Gradient evaluations: 53\n",
      "C=0.5727, gamma=100, Train error=0.0080, Test error=0.0060\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -317.8535954305744\n",
      "            Iterations: 23\n",
      "            Function evaluations: 20081\n",
      "            Gradient evaluations: 23\n",
      "C=0.8018, gamma=0.1, Train error=0.0000, Test error=0.2320\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -164.36301316918238\n",
      "            Iterations: 43\n",
      "            Function evaluations: 37545\n",
      "            Gradient evaluations: 43\n",
      "C=0.8018, gamma=0.5, Train error=0.0000, Test error=0.0100\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -104.21468043202346\n",
      "            Iterations: 52\n",
      "            Function evaluations: 45406\n",
      "            Gradient evaluations: 52\n",
      "C=0.8018, gamma=1, Train error=0.0000, Test error=0.0040\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.78833814662764\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55012\n",
      "            Gradient evaluations: 63\n",
      "C=0.8018, gamma=5, Train error=0.0000, Test error=0.0000\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -46.02604297959023\n",
      "            Iterations: 58\n",
      "            Function evaluations: 50642\n",
      "            Gradient evaluations: 58\n",
      "C=0.8018, gamma=100, Train error=0.0034, Test error=0.0040\n",
      "Best Parameters: C=0.5727, gamma=5\n",
      "Best Test Error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Cs = [100 / 873, 500 / 873, 700 / 873] #, 873\n",
    "gammas = [0.1, 0.5, 1, 5, 100]\n",
    "\n",
    "lowest_error = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "for C in Cs:\n",
    "    for gamma in gammas:\n",
    "        gsvm = GaussianSVM(C, gamma)\n",
    "        gsvm.fit(X_train, y_train)\n",
    "        sv_indices = gsvm.get_support_vectors()\n",
    "        train_error = gsvm.score(X_train, y_train)\n",
    "        test_error = gsvm.score(X_test, y_test)\n",
    "        print(f\"C={C:.4f}, gamma={gamma}, \n",
    "              Train error={train_error:.4f}, \n",
    "              Test error={test_error:.4f},\n",
    "              Number of Support Vectors: {len(sv_indices)}\")\n",
    "\n",
    "        if test_error < lowest_error:\n",
    "            lowest_error = test_error\n",
    "            best_params = (C, gamma)\n",
    "\n",
    "print(f\"Best Parameters: C={best_params[0]:.4f}, gamma={best_params[1]}\")\n",
    "print(f\"Best Test Error: {lowest_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -84.37393595325742\n",
      "            Iterations: 9\n",
      "            Function evaluations: 7858\n",
      "            Gradient evaluations: 9\n",
      "C=0.1145475372279496, Gamma=0.01, Number of Support Vectors: 872\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -82.72279493439663\n",
      "            Iterations: 15\n",
      "            Function evaluations: 13098\n",
      "            Gradient evaluations: 15\n",
      "C=0.1145475372279496, Gamma=0.1, Number of Support Vectors: 853\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -74.16551934430456\n",
      "            Iterations: 23\n",
      "            Function evaluations: 20084\n",
      "            Gradient evaluations: 23\n",
      "C=0.1145475372279496, Gamma=0.5, Number of Support Vectors: 745\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -63.41453356303385\n",
      "            Iterations: 25\n",
      "            Function evaluations: 21830\n",
      "            Gradient evaluations: 25\n",
      "C=0.1145475372279496, Gamma=1, Number of Support Vectors: 655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -26.45826553383153\n",
      "            Iterations: 36\n",
      "            Function evaluations: 31437\n",
      "            Gradient evaluations: 36\n",
      "C=0.1145475372279496, Gamma=5, Number of Support Vectors: 405\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -326.98871956857363\n",
      "            Iterations: 9\n",
      "            Function evaluations: 7858\n",
      "            Gradient evaluations: 9\n",
      "C=0.572737686139748, Gamma=0.01, Number of Support Vectors: 487\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -285.8328142581335\n",
      "            Iterations: 19\n",
      "            Function evaluations: 16590\n",
      "            Gradient evaluations: 19\n",
      "C=0.572737686139748, Gamma=0.1, Number of Support Vectors: 794\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -159.75505159853384\n",
      "            Iterations: 39\n",
      "            Function evaluations: 34053\n",
      "            Gradient evaluations: 39\n",
      "C=0.572737686139748, Gamma=0.5, Number of Support Vectors: 613\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -102.93934454458342\n",
      "            Iterations: 53\n",
      "            Function evaluations: 46277\n",
      "            Gradient evaluations: 53\n",
      "C=0.572737686139748, Gamma=1, Number of Support Vectors: 453\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.227984451198736\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55010\n",
      "            Gradient evaluations: 63\n",
      "C=0.572737686139748, Gamma=5, Number of Support Vectors: 172\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -391.3699737035605\n",
      "            Iterations: 11\n",
      "            Function evaluations: 9604\n",
      "            Gradient evaluations: 11\n",
      "C=0.8018327605956472, Gamma=0.01, Number of Support Vectors: 494\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -317.8535954305744\n",
      "            Iterations: 23\n",
      "            Function evaluations: 20081\n",
      "            Gradient evaluations: 23\n",
      "C=0.8018327605956472, Gamma=0.1, Number of Support Vectors: 652\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -164.36301316918238\n",
      "            Iterations: 43\n",
      "            Function evaluations: 37545\n",
      "            Gradient evaluations: 43\n",
      "C=0.8018327605956472, Gamma=0.5, Number of Support Vectors: 666\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -104.21468043202346\n",
      "            Iterations: 52\n",
      "            Function evaluations: 45406\n",
      "            Gradient evaluations: 52\n",
      "C=0.8018327605956472, Gamma=1, Number of Support Vectors: 525\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.78833814662764\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55012\n",
      "            Gradient evaluations: 63\n",
      "C=0.8018327605956472, Gamma=5, Number of Support Vectors: 184\n"
     ]
    }
   ],
   "source": [
    "#list the number of support vector for each C\n",
    "np.random.seed(0)\n",
    "# Parameters\n",
    "Cs = [100 / 873, 500 / 873, 700 / 873] \n",
    "gammas = [0.01, 0.1, 0.5, 1, 5]\n",
    "\n",
    "# Track support vectors\n",
    "support_vectors = {}\n",
    "for C in Cs:\n",
    "    for gamma in gammas:\n",
    "        gsvm = GaussianSVM(C=C, gamma=gamma)\n",
    "        gsvm.fit(X_train, y_train)\n",
    "        sv_indices = gsvm.get_support_vectors()\n",
    "        support_vectors[gamma] = sv_indices\n",
    "        print(f\"C={C}, Gamma={gamma}, Number of Support Vectors: {len(sv_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gamma \u001b[38;5;129;01min\u001b[39;00m gammas:\n\u001b[0;32m     10\u001b[0m     gsvm \u001b[38;5;241m=\u001b[39m GaussianSVM(C\u001b[38;5;241m=\u001b[39mC, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mgsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     sv_indices \u001b[38;5;241m=\u001b[39m gsvm\u001b[38;5;241m.\u001b[39mget_support_vectors()\n\u001b[0;32m     13\u001b[0m     support_vectors[gamma] \u001b[38;5;241m=\u001b[39m sv_indices\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mGaussianSVM.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     47\u001b[0m alpha0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_samples)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Solve the optimization problem\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSLSQP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Extract the optimal alpha\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    702\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 705\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_slsqp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    708\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    709\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    710\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:432\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    429\u001b[0m     c \u001b[38;5;241m=\u001b[39m _eval_constraint(x, cons)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# gradient evaluation required\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m     g \u001b[38;5;241m=\u001b[39m append(\u001b[43mwrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    433\u001b[0m     a \u001b[38;5;241m=\u001b[39m _eval_con_normals(x, cons, la, n, m, meq, mieq)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m majiter \u001b[38;5;241m>\u001b[39m majiter_prev:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;66;03m# call callback if major iteration has incremented\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_optimize.py:346\u001b[0m, in \u001b[0;36m_clip_x_for_func.<locals>.eval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(x):\n\u001b[0;32m    345\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_clip_x(x, bounds)\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:273\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[0;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mGaussianSVM.fit.<locals>.objective\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(alpha):\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(alpha) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m K)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Parameters\n",
    "C = 500 / 873 \n",
    "gammas = [0.01, 0.1, 0.5, 1, 5]\n",
    "\n",
    "# Track support vectors\n",
    "support_vectors = {}\n",
    "for gamma in gammas:\n",
    "    gsvm = GaussianSVM(C=C, gamma=gamma)\n",
    "    gsvm.fit(X_train, y_train)\n",
    "    sv_indices = gsvm.get_support_vectors()\n",
    "    support_vectors[gamma] = sv_indices\n",
    "    print(f\"Gamma={gamma}, Number of Support Vectors: {len(sv_indices)}\")\n",
    "\n",
    "# Calculate overlaps between consecutive gammas\n",
    "for i in range(len(gammas) - 1):\n",
    "    gamma1, gamma2 = gammas[i], gammas[i + 1]\n",
    "    overlap = len(np.intersect1d(support_vectors[gamma1], support_vectors[gamma2]))\n",
    "    print(f\"Overlap between gamma={gamma1} and gamma={gamma2}: {overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KernelPerceptron:\n",
    "    def __init__(self, gamma, max_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.max_epochs = max_epochs\n",
    "        self.c = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def gaussian_kernel(self, x1, x2):\n",
    "        return np.exp(-np.linalg.norm(x1 - x2)**2 / self.gamma)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N = X.shape[0]\n",
    "        self.c = np.zeros(N)  # Mistake counts\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            for i in range(N):\n",
    "                # Compute decision function\n",
    "                decision = sum(self.c[j] * y[j] * self.gaussian_kernel(X[j], X[i]) for j in range(N))\n",
    "                if y[i] * decision <= 0:  # Misclassified\n",
    "                    self.c[i] += 1  # Update mistake count\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            # Compute decision function for new point\n",
    "            decision = sum(self.c[i] * self.y_train[i] * self.gaussian_kernel(self.X_train[i], x) for i in range(len(self.X_train)))\n",
    "            y_pred.append(np.sign(decision))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)  # Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma=0.1, Train Accuracy=0.0000, Test Accuracy=0.0020\n",
      "Gamma=0.5, Train Accuracy=0.0000, Test Accuracy=0.0040\n",
      "Gamma=1, Train Accuracy=0.0000, Test Accuracy=0.0040\n",
      "Gamma=5, Train Accuracy=0.0000, Test Accuracy=0.0040\n",
      "Gamma=100, Train Accuracy=0.0000, Test Accuracy=0.0000\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "# Test different gamma values\n",
    "gammas = [0.1, 0.5, 1, 5, 100]\n",
    "for gamma in gammas:\n",
    "    kp = KernelPerceptron(gamma=gamma)\n",
    "    kp.fit(X_train, y_train)\n",
    "    train_error = kp.score(X_train, y_train)\n",
    "    test_error = kp.score(X_test, y_test)\n",
    "    print(f\"Gamma={gamma}, Train error ={train_error:.4f}, Test error ={test_error:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
