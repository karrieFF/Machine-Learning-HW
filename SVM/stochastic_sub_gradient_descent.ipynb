{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload dataset\n",
    "train_data = pd.read_csv(\"D:/EIC-Code/00-Python/Machine-Learning-HW/SVM/bank-note/train.csv\", header = None, names = ['variance','skewness','curtosis','entropy','y'])\n",
    "test_data = pd.read_csv(\"D:/EIC-Code/00-Python/Machine-Learning-HW/SVM/bank-note/test.csv\", names = ['variance','skewness','curtosis','entropy','y'])\n",
    "\n",
    "features = ['variance','skewness','curtosis','entropy']\n",
    "outcome = 'y'\n",
    "\n",
    "X_train = train_data[features].values #change to matrix multiple\n",
    "y_train = train_data[outcome].values\n",
    "X_test = test_data[features].values\n",
    "y_test = test_data[outcome].values\n",
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimalSVM:\n",
    "    def __init__(self, gamma, a, C, N):\n",
    "        self.gamma = gamma\n",
    "        self.a = a\n",
    "        self.C = C\n",
    "        self.N = N\n",
    "        self.w = None  # Weight vector\n",
    "        self.b = 0  # Bias term\n",
    "        self.objective_curve = []  # Stores hinge loss at each epoch\n",
    "\n",
    "    def _hinge_loss(self, X, y):\n",
    "        loss = 0.5 * np.dot(self.w, self.w) + self.C * np.sum(np.maximum(0, 1 - y * (np.dot(X, self.w) + self.b))) #objective function\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs, schedule):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.w = np.zeros(n_features)  # Initialize weights\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            perm = np.random.permutation(n_samples)\n",
    "            X_train, y_train = X_train[perm], y_train[perm]\n",
    "\n",
    "            for i, (xi, yi) in enumerate(zip(X_train, y_train)):\n",
    "                t = epoch * n_samples + i + 1  # Global step count\n",
    "\n",
    "                # Learning rate schedule\n",
    "                if schedule == \"schedule1\":\n",
    "                    eta_t = self.gamma / (1 + (self.gamma / self.a) * t)\n",
    "                elif schedule == \"schedule2\":\n",
    "                    eta_t = self.gamma / (1 + t)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid schedule. Choose 'schedule1' or 'schedule2'.\")\n",
    "\n",
    "                # Sub-gradient updates\n",
    "                if yi * (np.dot(self.w, xi) + self.b) <= 1:\n",
    "                    self.w = (1 - eta_t) * self.w + eta_t * self.C * self.N * yi * xi\n",
    "                    self.b += eta_t * self.C * yi\n",
    "                else:\n",
    "                    self.w = (1 - eta_t) * self.w\n",
    "\n",
    "            # Compute hinge loss at the end of each epoch\n",
    "            self.objective_curve.append(self._hinge_loss(X_train, y_train))\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)\n",
    "\n",
    "    def get_objective_curve(self):\n",
    "        return self.objective_curve\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "    \n",
    "    def get_bias(self):\n",
    "        return self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters for schedule1:\n",
      "C=0.114548, Schedule: schedule1, gamma=0.5, a=100, train_error: 0.0608, test_error: 0.0840, weights: [-0.15217176 -0.11147137 -0.05579284 -0.01562807], bias: 0.14298246924389865\n",
      "C=0.572738, Schedule: schedule1, gamma=0.01, a=1, train_error: 0.0241, test_error: 0.0180, weights: [-0.32441602 -0.17212906 -0.18608215 -0.02517079], bias: 0.5004602638064877\n",
      "C=0.801833, Schedule: schedule1, gamma=0.001, a=100, train_error: 0.0183, test_error: 0.0160, weights: [-0.34762413 -0.18979735 -0.22631406 -0.00247417], bias: 0.6468729605176408\n",
      "\n",
      "Best Parameters for schedule2:\n",
      "C=0.114548, Schedule: schedule2, gamma=1, a=50, train_error: 0.0631, test_error: 0.0860, weights: [-0.15368874 -0.11439085 -0.05609331 -0.01529727], bias: 0.1352745796472408\n",
      "C=0.572738, Schedule: schedule2, gamma=1, a=10, train_error: 0.0218, test_error: 0.0160, weights: [-0.32397074 -0.17102877 -0.18428299 -0.02530887], bias: 0.4902974613772339\n",
      "C=0.801833, Schedule: schedule2, gamma=1, a=10, train_error: 0.0241, test_error: 0.0180, weights: [-0.3530895  -0.18560306 -0.21316165 -0.01371438], bias: 0.5985959503707144\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "Cs = [100 / 873, 500 / 873, 700 / 873]\n",
    "gamma_values = [1, 0.5, 0.1, 0.01, 0.001]\n",
    "a_values = [1, 10, 50, 100]\n",
    "N = 1\n",
    "epochs = 100\n",
    "\n",
    "# Initialize variables to track the best parameters and lowest error for each schedule and C\n",
    "best_params_per_schedule = {\"schedule1\": {}, \"schedule2\": {}}\n",
    "\n",
    "# Perform grid search\n",
    "for schedule in [\"schedule1\", \"schedule2\"]:\n",
    "    for C in Cs:\n",
    "        lowest_error_for_C = float(\"inf\")\n",
    "        best_params_for_C = None\n",
    "        for gamma in gamma_values:\n",
    "            for a in a_values:\n",
    "                # Initialize and train the SVM\n",
    "                svm = PrimalSVM(gamma=gamma, a=a, C=C, N=N)\n",
    "                svm.fit(X_train, y_train, epochs=epochs, schedule=schedule)\n",
    "\n",
    "                # Calculate training and test errors\n",
    "                train_error = svm.score(X_train, y_train)\n",
    "                test_error = svm.score(X_test, y_test)\n",
    "                weights = svm.get_weights()\n",
    "                bias = svm.get_bias()\n",
    "\n",
    "                #print(f\"Schedule: {schedule}, C={C:.6f}, gamma={gamma}, a={a}, \"\n",
    "                #f\"train_error: {train_error:.4f}, test_error: {test_error:.4f},\"\n",
    "                #f\"weights, {weights}, bias = {bias}\")\n",
    "\n",
    "                #output objective curve \n",
    "                #objective_curve = svm.get_objective_curve()\n",
    "                #plt.figure(figsize=(8, 5))\n",
    "                #plt.plot(range(1, len(objective_curve) + 1), objective_curve, marker='o')\n",
    "                #plt.title(\"Objective Function Curve (Hinge Loss)\")\n",
    "                #plt.xlabel(\"Epoch\")\n",
    "                #plt.ylabel(\"Hinge Loss\")\n",
    "                #plt.grid(True)\n",
    "                #plt.show()\n",
    "\n",
    "                # Update the best parameters for the current C\n",
    "                if test_error < lowest_error_for_C:\n",
    "                    lowest_error_for_C = test_error\n",
    "                    best_params_for_C = {\n",
    "                        \"C\": C,\n",
    "                        \"gamma\": gamma,\n",
    "                        \"a\": a,\n",
    "                        \"schedule\": schedule,\n",
    "                        \"train_error\": train_error,\n",
    "                        \"test_error\": test_error,\n",
    "                        \"weights\": weights,\n",
    "                        \"bias\": bias\n",
    "                    }\n",
    "        \n",
    "        # Store the best parameters for the current C and schedule\n",
    "        best_params_per_schedule[schedule][C] = best_params_for_C\n",
    "\n",
    "# Print the best parameters for each C, separated by schedule\n",
    "for schedule, params_per_C in best_params_per_schedule.items():\n",
    "    print(f\"\\nBest Parameters for {schedule}:\")\n",
    "    for C, params in params_per_C.items():\n",
    "        print(f\"C={C:.6f}, Schedule: {params['schedule']}, gamma={params['gamma']}, a={params['a']}, \"\n",
    "              f\"train_error: {params['train_error']:.4f}, test_error: {params['test_error']:.4f}, \"\n",
    "              f\"weights: {params['weights']}, bias: {params['bias']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualSVM:\n",
    "    def __init__(self, C):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.alpha = None  # Lagrange multipliers\n",
    "        self.w = None  # Weight vector\n",
    "        self.b = None  # Bias term\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # kernel matrix (linear kernel)\n",
    "        K = np.dot(X, X.T)\n",
    "\n",
    "        # define the dual objective function\n",
    "        def objective(alpha):\n",
    "            return -np.sum(alpha) + 0.5 * np.sum((alpha * y)[:, None] * (alpha * y) * K)\n",
    "\n",
    "        # Initial guess for alpha\n",
    "        alpha0 = np.zeros(n_samples)\n",
    "\n",
    "        # Bounds for alpha: 0 <= alpha <= C\n",
    "        bounds = [(0, C) for _ in range(n_samples)]\n",
    "\n",
    "        # Equality constraint: sum(alpha * y) = 0\n",
    "        constraints = {\n",
    "            'type': 'eq',\n",
    "            'fun': lambda alpha: np.dot(alpha, y),\n",
    "            'jac': lambda alpha: y\n",
    "            }\n",
    "        \n",
    "        # Solve the optimization problem\n",
    "        result = minimize(\n",
    "            objective,\n",
    "            alpha0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints\n",
    "        )\n",
    "\n",
    "        # extract the optimal alpha\n",
    "        self.alpha = result.x #minimize the function to get alpha\n",
    "\n",
    "        # compute weight vector\n",
    "        self.w = np.sum((self.alpha * y)[:, None] * X, axis=0)\n",
    "\n",
    "        # compute bias term\n",
    "        support_vector_idx = np.where((self.alpha > 0) & (self.alpha < self.C))[0][0]\n",
    "        self.b = y[support_vector_idx] - np.dot(self.w, X[support_vector_idx])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.1145475372279496, Train Accuracy=0.11, Test Accuracy=0.12\n",
      "Weights:, [-0.94292598 -0.65149184 -0.73372197 -0.04102195], Bias:, 4.14119177534919\n",
      "C=0.572737686139748, Train Accuracy=0.15, Test Accuracy=0.14\n",
      "Weights:, [-1.56393784 -1.01405165 -1.18065044 -0.15651687], Bias:, 7.590350666124916\n",
      "C=0.8018327605956472, Train Accuracy=0.40, Test Accuracy=0.43\n",
      "Weights:, [-2.04254833 -1.28068891 -1.51351532 -0.24905307], Bias:, 12.975949611428163\n",
      "Weights: [-2.04254833 -1.28068891 -1.51351532 -0.24905307]\n",
      "Bias: 12.975949611428163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define hyperparameter C\n",
    "Cs = [100 / 873, 500 / 873, 700 / 873] #, 873\n",
    "\n",
    "# Train and evaluate the model for different values of C\n",
    "for C in Cs:\n",
    "    svm = DualSVM(C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    train_error = svm.score(X_train, y_train)\n",
    "    test_error = svm.score(X_test, y_test)\n",
    "    print(f\"C={C}, Train error={train_error:.2f}, Test error={test_error:.2f}\")\n",
    "    print(f\"Weights:, {svm.w}, Bias:, {svm.b}\")\n",
    "\n",
    "# Print the weights and bias for the best model\n",
    "print(\"Weights:\", svm.w)\n",
    "print(\"Bias:\", svm.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSVM:\n",
    "    def __init__(self, C, gamma):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.alpha = None  # lagrange multipliers\n",
    "        self.b = None  # bias term\n",
    "        self.X_train = None  # training features\n",
    "        self.y_train = None  # training labels\n",
    "\n",
    "    def gaussian_kernel(self, x1, x2):\n",
    "        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / self.gamma)\n",
    "\n",
    "    def kernel_matrix(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i, j] = self.gaussian_kernel(X[i], X[j])\n",
    "        return K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Compute the kernel matrix\n",
    "        K = self.kernel_matrix(X)\n",
    "\n",
    "        # Define the dual objective function\n",
    "        def objective(alpha):\n",
    "            return -np.sum(alpha) + 0.5 * np.sum((alpha * y)[:, None] * (alpha * y) * K)\n",
    "\n",
    "        # Bounds for alpha: 0 <= alpha <= C\n",
    "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
    "\n",
    "        # Equality constraint: sum(alpha * y) = 0\n",
    "        constraints = {\n",
    "            'type': 'eq',\n",
    "            'fun': lambda alpha: np.dot(alpha, y),\n",
    "            'jac': lambda alpha: y\n",
    "        }\n",
    "\n",
    "        # Initial guess for alpha\n",
    "        alpha0 = np.zeros(n_samples)\n",
    "\n",
    "        # Solve the optimization problem\n",
    "        result = minimize(\n",
    "            objective,\n",
    "            alpha0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            options={'maxiter': 1000, 'disp': True}\n",
    "        )\n",
    "\n",
    "        # Extract the optimal alpha\n",
    "        self.alpha = result.x\n",
    "\n",
    "        # Compute bias term using support vectors\n",
    "        support_vector_idx = np.where((self.alpha > 1e-4) & (self.alpha < self.C))[0]\n",
    "        support_vector_idx = support_vector_idx[0]\n",
    "        self.b = y[support_vector_idx] - np.sum(self.alpha * y * K[support_vector_idx])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            # Decision function\n",
    "            decision = np.sum(\n",
    "                self.alpha * self.y_train *\n",
    "                np.array([self.gaussian_kernel(x, x_train) for x_train in self.X_train])\n",
    "            ) + self.b\n",
    "            y_pred.append(np.sign(decision))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)  # Error rate\n",
    "\n",
    "    def get_support_vectors(self):\n",
    "        return np.where((self.alpha > 1e-4) & (self.alpha < self.C))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -82.72279493439663\n",
      "            Iterations: 15\n",
      "            Function evaluations: 13098\n",
      "            Gradient evaluations: 15\n",
      "C=0.1145, gamma=0.1, Train error=0.4461, Test error=0.4420, Number of Support Vectors: 853\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -74.16551934430456\n",
      "            Iterations: 23\n",
      "            Function evaluations: 20084\n",
      "            Gradient evaluations: 23\n",
      "C=0.1145, gamma=0.5, Train error=0.4071, Test error=0.4260, Number of Support Vectors: 745\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -63.41453356303385\n",
      "            Iterations: 25\n",
      "            Function evaluations: 21830\n",
      "            Gradient evaluations: 25\n",
      "C=0.1145, gamma=1, Train error=0.2144, Test error=0.2720, Number of Support Vectors: 655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\scipy\\optimize\\_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -26.45826553383153\n",
      "            Iterations: 36\n",
      "            Function evaluations: 31437\n",
      "            Gradient evaluations: 36\n",
      "C=0.1145, gamma=5, Train error=0.0034, Test error=0.0040, Number of Support Vectors: 405\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -20.289226978400574\n",
      "            Iterations: 26\n",
      "            Function evaluations: 22703\n",
      "            Gradient evaluations: 26\n",
      "C=0.1145, gamma=100, Train error=0.0138, Test error=0.0080, Number of Support Vectors: 38\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -285.8328142581335\n",
      "            Iterations: 19\n",
      "            Function evaluations: 16590\n",
      "            Gradient evaluations: 19\n",
      "C=0.5727, gamma=0.1, Train error=0.0000, Test error=0.3480, Number of Support Vectors: 794\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -159.75505159853384\n",
      "            Iterations: 39\n",
      "            Function evaluations: 34053\n",
      "            Gradient evaluations: 39\n",
      "C=0.5727, gamma=0.5, Train error=0.0000, Test error=0.0180, Number of Support Vectors: 613\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -102.93934454458342\n",
      "            Iterations: 53\n",
      "            Function evaluations: 46277\n",
      "            Gradient evaluations: 53\n",
      "C=0.5727, gamma=1, Train error=0.0000, Test error=0.0040, Number of Support Vectors: 453\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.227984451198736\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55010\n",
      "            Gradient evaluations: 63\n",
      "C=0.5727, gamma=5, Train error=0.0000, Test error=0.0000, Number of Support Vectors: 172\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -39.83017265028401\n",
      "            Iterations: 53\n",
      "            Function evaluations: 46277\n",
      "            Gradient evaluations: 53\n",
      "C=0.5727, gamma=100, Train error=0.0080, Test error=0.0060, Number of Support Vectors: 19\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -317.8535954305744\n",
      "            Iterations: 23\n",
      "            Function evaluations: 20081\n",
      "            Gradient evaluations: 23\n",
      "C=0.8018, gamma=0.1, Train error=0.0000, Test error=0.2320, Number of Support Vectors: 652\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -164.36301316918238\n",
      "            Iterations: 43\n",
      "            Function evaluations: 37545\n",
      "            Gradient evaluations: 43\n",
      "C=0.8018, gamma=0.5, Train error=0.0000, Test error=0.0100, Number of Support Vectors: 666\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -104.21468043202346\n",
      "            Iterations: 52\n",
      "            Function evaluations: 45406\n",
      "            Gradient evaluations: 52\n",
      "C=0.8018, gamma=1, Train error=0.0000, Test error=0.0040, Number of Support Vectors: 525\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.78833814662764\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55012\n",
      "            Gradient evaluations: 63\n",
      "C=0.8018, gamma=5, Train error=0.0000, Test error=0.0000, Number of Support Vectors: 184\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -46.02604297959023\n",
      "            Iterations: 58\n",
      "            Function evaluations: 50642\n",
      "            Gradient evaluations: 58\n",
      "C=0.8018, gamma=100, Train error=0.0034, Test error=0.0040, Number of Support Vectors: 41\n",
      "Best Parameters: C=0.5727, gamma=5\n",
      "Best Test Error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "Cs = [100 / 873, 500 / 873, 700 / 873] #, 873\n",
    "gammas = [0.1, 0.5, 1, 5, 100]\n",
    "\n",
    "lowest_error = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "for C in Cs:\n",
    "    for gamma in gammas:\n",
    "        gsvm = GaussianSVM(C, gamma)\n",
    "        gsvm.fit(X_train, y_train)\n",
    "        sv_indices = gsvm.get_support_vectors()\n",
    "        train_error = gsvm.score(X_train, y_train)\n",
    "        test_error = gsvm.score(X_test, y_test)\n",
    "        print(f\"C={C:.4f}, gamma={gamma}, Train error={train_error:.4f}, Test error={test_error:.4f}, Number of Support Vectors: {len(sv_indices)}\")\n",
    "\n",
    "        if test_error < lowest_error:\n",
    "            lowest_error = test_error\n",
    "            best_params = (C, gamma)\n",
    "\n",
    "print(f\"Best Parameters: C={best_params[0]:.4f}, gamma={best_params[1]}\")\n",
    "print(f\"Best Test Error: {lowest_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -326.98871956857363\n",
      "            Iterations: 9\n",
      "            Function evaluations: 7858\n",
      "            Gradient evaluations: 9\n",
      "Gamma=0.01, Number of Support Vectors: 487\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -285.8328142581335\n",
      "            Iterations: 19\n",
      "            Function evaluations: 16590\n",
      "            Gradient evaluations: 19\n",
      "Gamma=0.1, Number of Support Vectors: 794\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -159.75505159853384\n",
      "            Iterations: 39\n",
      "            Function evaluations: 34053\n",
      "            Gradient evaluations: 39\n",
      "Gamma=0.5, Number of Support Vectors: 613\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -102.93934454458342\n",
      "            Iterations: 53\n",
      "            Function evaluations: 46277\n",
      "            Gradient evaluations: 53\n",
      "Gamma=1, Number of Support Vectors: 453\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.227984451198736\n",
      "            Iterations: 63\n",
      "            Function evaluations: 55010\n",
      "            Gradient evaluations: 63\n",
      "Gamma=5, Number of Support Vectors: 172\n",
      "Overlap between gamma=0.01 and gamma=0.1: 480\n",
      "Overlap between gamma=0.1 and gamma=0.5: 570\n",
      "Overlap between gamma=0.5 and gamma=1: 375\n",
      "Overlap between gamma=1 and gamma=5: 105\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "C = 500 / 873 \n",
    "gammas = [0.01, 0.1, 0.5, 1, 5]\n",
    "\n",
    "# Track support vectors\n",
    "support_vectors = {}\n",
    "for gamma in gammas:\n",
    "    gsvm = GaussianSVM(C=C, gamma=gamma)\n",
    "    gsvm.fit(X_train, y_train)\n",
    "    sv_indices = gsvm.get_support_vectors()\n",
    "    support_vectors[gamma] = sv_indices\n",
    "    print(f\"Gamma={gamma}, Number of Support Vectors: {len(sv_indices)}\")\n",
    "\n",
    "# Calculate overlaps between consecutive gammas\n",
    "for i in range(len(gammas) - 1):\n",
    "    gamma1, gamma2 = gammas[i], gammas[i + 1]\n",
    "    overlap = len(np.intersect1d(support_vectors[gamma1], support_vectors[gamma2]))\n",
    "    print(f\"Overlap between gamma={gamma1} and gamma={gamma2}: {overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, gamma, max_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.max_epochs = max_epochs\n",
    "        self.c = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def gaussian_kernel(self, x1, x2):\n",
    "        return np.exp(-np.linalg.norm(x1 - x2)**2 / self.gamma)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N = X.shape[0]\n",
    "        self.c = np.zeros(N)  # Mistake counts\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            for i in range(N):\n",
    "                # Compute decision function\n",
    "                decision = sum(self.c[j] * y[j] * self.gaussian_kernel(X[j], X[i]) for j in range(N))\n",
    "                if y[i] * decision <= 0:  # Misclassified\n",
    "                    self.c[i] += 1  # Update mistake count\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            # Compute decision function for new point\n",
    "            decision = sum(self.c[i] * self.y_train[i] * self.gaussian_kernel(self.X_train[i], x) for i in range(len(self.X_train)))\n",
    "            y_pred.append(np.sign(decision))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions != y)  # Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma=0.1, Train error =0.0000, Test error =0.0020\n",
      "Gamma=0.5, Train error =0.0000, Test error =0.0040\n",
      "Gamma=1, Train error =0.0000, Test error =0.0040\n",
      "Gamma=5, Train error =0.0000, Test error =0.0040\n",
      "Gamma=100, Train error =0.0000, Test error =0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test different gamma values\n",
    "gammas = [0.1, 0.5, 1, 5, 100]\n",
    "for gamma in gammas:\n",
    "    kp = KernelPerceptron(gamma=gamma)\n",
    "    kp.fit(X_train, y_train)\n",
    "    train_error = kp.score(X_train, y_train)\n",
    "    test_error = kp.score(X_test, y_test)\n",
    "    print(f\"Gamma={gamma}, Train error ={train_error:.4f}, Test error ={test_error:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
